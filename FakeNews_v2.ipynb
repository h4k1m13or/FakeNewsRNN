{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T04:29:11.411489Z",
     "start_time": "2020-09-13T04:29:11.405488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40494,
     "status": "ok",
     "timestamp": 1600473093462,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "cJOQtgjlCslS",
    "outputId": "0fe89e0d-40f0-4fa0-94c0-482f31357dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Mounted at /content/drive\n",
      "/content/drive/Shared drives/AARN/Fake-News-RNN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/Shared drives/AARN/Fake-News-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0OkYy4sCslb"
   },
   "source": [
    "# 1-Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T04:29:16.871945Z",
     "start_time": "2020-09-13T04:29:15.413449Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47257,
     "status": "ok",
     "timestamp": 1600473100357,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "OCeXxkGOCslc",
    "outputId": "8285b57e-c24f-44d8-d45a-1f77fb689a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data\t\t\t     FakeNews_v2.ipynb\t saved\n",
      "'Fake News detector.ipynb'   FakeNews-v2.ipynb\n",
      "index        0\n",
      "author    2460\n",
      "id           0\n",
      "label        0\n",
      "text        46\n",
      "title      680\n",
      "dtype: int64\n",
      "22860\n"
     ]
    }
   ],
   "source": [
    "#inspecting our data\n",
    "! ls\n",
    "train_set='./data/train.csv'\n",
    "test_set='data/test.csv'\n",
    "submit='data/submit.csv'\n",
    "\n",
    "df_train=pd.read_csv(train_set)\n",
    "df_test=pd.read_csv(test_set)\n",
    "df_submit=pd.read_csv(submit)\n",
    "#concatinate test with submit\n",
    "#print(df_submit)\n",
    "df_test['label']=df_submit['label'].values\n",
    "#print(df_test)\n",
    "\n",
    "#concatinate test with train set, we will split them later \n",
    "df=pd.concat([df_train,df_test],ignore_index=True,sort=True,).reset_index()\n",
    "print(df.isnull().sum()) # how much null values we have for each collumn\n",
    "df=df.dropna() # delete null values\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T07:01:54.104561Z",
     "start_time": "2020-09-13T04:39:02.553502Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 597258,
     "status": "ok",
     "timestamp": 1600473650373,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "R91-n7E3Csll",
    "outputId": "13199b12-ea15-4fd5-9dad-198ed663540a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  ...                                          titletext\n",
      "0      1  ...  House Dem Aide : We Didn ’ t Even See Comey ’ ...\n",
      "1      0  ...  FLYNN : Hillary Clinton , Big Woman on Campus ...\n",
      "2      1  ...  Why the Truth Might Get You Fired . Why the Tr...\n",
      "3      1  ...  15 Civilians Killed In Single US Airstrike Hav...\n",
      "4      1  ...  Iranian woman jail for fictional unpublished s...\n",
      "\n",
      "[5 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(text):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  punctuations=\"?:!.,;\"\n",
    "  sentence_words = nltk.word_tokenize(text)\n",
    "  lemmas=[]\n",
    "  for word, tag in pos_tag(word_tokenize(text)):\n",
    "      wntag = tag[0].lower()\n",
    "      wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "      if not wntag:\n",
    "              lemma = word\n",
    "      else:\n",
    "             lemma = wnl.lemmatize(word, wntag)\n",
    "      lemmas.append(lemma)\n",
    "  \n",
    "  \n",
    "\n",
    "  return \" \".join(lemmas)\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)  \n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.stem(word) for word in text if not word in stopwords.words(\"english\")]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def trimming(text):\n",
    "    text = text.split(maxsplit=600)\n",
    "    text = ' '.join(text[:600])\n",
    "    return text\n",
    "\n",
    "\n",
    "df['title']=df['title'].apply(lambda x: trimming(x))\n",
    "df['title']=df['title'].apply(lambda x: lemmatization(x))\n",
    "#df['title']=df['title'].apply(lambda x: stemming(x))\n",
    "df['text']=df['text'].apply(lambda x: trimming(x))\n",
    "df['text']=df['text'].apply(lambda x: lemmatization(x))\n",
    "#df['text']=df['text'].apply(lambda x: stemming(x))\n",
    "\n",
    "#df['titletext']=df['titletext'].apply(lambda x: lemmatization(x))\n",
    "#df['titletext']=df['titletext'].apply(lambda x: stemming(x))\n",
    "df['titletext'] = df['title'] + \" . \" + df['text']\n",
    "df['titletext']=df['titletext'].apply(lambda x: trimming(x))\n",
    "df = df.reindex(columns=['label', 'title', 'text', 'titletext'])\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1600474264186,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "nUm0EJjCrtM-",
    "outputId": "2bacfc20-8280-4d4b-e280-05fe6e8ae8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed  test.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T09:30:46.826131Z",
     "start_time": "2020-09-13T09:30:42.593712Z"
    },
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7448,
     "status": "ok",
     "timestamp": 1600474275625,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "32_8BWb6Cslq"
   },
   "outputs": [],
   "source": [
    "df.drop( df[df.text.str.len() < 100].index, inplace=True) # drop short texts\n",
    "\n",
    "# Split by label\n",
    "df_real = df[df['label'] == 0]\n",
    "df_fake = df[df['label'] == 1]\n",
    "\n",
    "# Train-test split\n",
    "df_real_full_train, df_real_test = train_test_split(df_real, train_size = 0.7,test_size=0.3, random_state = 102)\n",
    "df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = 0.7,test_size=0.3, random_state = 102)\n",
    "\n",
    "# Train-valid split\n",
    "df_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = 0.7,test_size=0.3, random_state = 102)\n",
    "df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = 0.7,test_size=0.3, random_state = 102)\n",
    "\n",
    "# Concatenate splits of different labels\n",
    "df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\n",
    "df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\n",
    "df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n",
    "\n",
    "# Write preprocessed data\n",
    "df_train.to_csv( './data/preprocessed/train.csv', index=False)\n",
    "df_valid.to_csv('./data/preprocessed/valid.csv', index=False)\n",
    "df_test.to_csv( './data/preprocessed/test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbNIrC8nCslw"
   },
   "source": [
    "# 2- loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T09:31:12.056314Z",
     "start_time": "2020-09-13T09:30:53.471590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4320,
     "status": "ok",
     "timestamp": 1600474289102,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "LX9Yl36SCslx",
    "outputId": "0e5b8450-6bd8-4acb-d743-ec4eb557fe8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T09:41:55.297478Z",
     "start_time": "2020-09-13T09:40:27.083231Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103318,
     "status": "ok",
     "timestamp": 1600474388121,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "ViC4dHIZCsl4",
    "outputId": "1046b69a-ef92-45d6-c6ea-6a55871fc757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 24.4MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 20kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 30kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 40kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 51kB 6.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 61kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 71kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 81kB 7.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 92kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 102kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 112kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 122kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 133kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 143kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 153kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 163kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 174kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 184kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 194kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 204kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 215kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 225kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 235kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 245kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 256kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 266kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 276kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 286kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 296kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 307kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 317kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 327kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 337kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 348kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 358kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 368kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 378kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 389kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 399kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 409kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 419kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 430kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 440kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 450kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 460kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 471kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 481kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 491kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 501kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 512kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 522kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 532kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 542kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 552kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 563kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 573kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 583kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 593kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 604kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 614kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 624kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 634kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 645kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 655kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 665kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 675kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 686kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 696kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 706kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 716kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 727kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 737kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 747kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 757kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 768kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 778kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 788kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 798kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 808kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 819kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 829kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 839kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 849kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 860kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 870kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 880kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 890kB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7d0c24eb45d272010692a09c3c2b8a8cb7d4d10374403ebf0d9ee0fab0fe8d48\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.0.43\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(tokenize='moses', lower=True, include_lengths=True, batch_first=True)\n",
    "fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path='./data/preprocessed/', train='train.csv', validation='valid.csv', test='test.csv',\n",
    "                                           format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "# Iterators\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=64, sort_key=lambda x: len(x.text),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=64, sort_key=lambda x: len(x.text),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=64, sort_key=lambda x: len(x.text),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "\n",
    "# Vocabulary\n",
    "\n",
    "text_field.build_vocab(train, min_freq=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8V5TC6tVCsl9"
   },
   "source": [
    "# 3- Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T09:42:00.756328Z",
     "start_time": "2020-09-13T09:42:00.744301Z"
    },
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103307,
     "status": "ok",
     "timestamp": 1600474388125,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "nl8opUfeCsl-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T10:17:07.078341Z",
     "start_time": "2020-09-13T10:17:07.067333Z"
    },
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103287,
     "status": "ok",
     "timestamp": 1600474388125,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "kpcgQhavCsmF"
   },
   "outputs": [],
   "source": [
    "class FakeNewsNet(nn.Module):\n",
    "    def __init__(self,vocab_size=len(text_field.vocab),hidden_size=100,num_layers=1,bi_lstm=False):\n",
    "        super(FakeNewsNet, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.bi_lstm=bi_lstm\n",
    "        self.embedding = nn.Embedding(self.vocab_size,256)\n",
    "        self.LSTM = nn.LSTM(input_size=256,hidden_size=self.hidden_size,num_layers=self.num_layers,bidirectional=self.bi_lstm,batch_first=True)\n",
    "        self.drop= nn.Dropout(p=0.5)\n",
    "        if bi_lstm:\n",
    "          self.out = nn.Linear(2*self.hidden_size, 1)\n",
    "        else:\n",
    "          self.out = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, inp, input_len):\n",
    "\n",
    "        embeded_text = self.embedding(inp)\n",
    "        packed_input = pack_padded_sequence(embeded_text, input_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.LSTM(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), input_len - 1, :self.hidden_size]\n",
    "        out_reverse = output[:, 0, self.hidden_size:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.out(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UzHGf0ayCsmK"
   },
   "source": [
    "# 4-Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T10:17:09.044124Z",
     "start_time": "2020-09-13T10:17:09.035118Z"
    },
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103267,
     "status": "ok",
     "timestamp": 1600474388126,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "JJjIazhwCsmL"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to :{save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from : {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to: {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from: {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T10:17:12.165511Z",
     "start_time": "2020-09-13T10:17:12.162509Z"
    },
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103252,
     "status": "ok",
     "timestamp": 1600474388126,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "M4SnM5xlCsmP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-13T10:17:13.595Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2619790,
     "status": "ok",
     "timestamp": 1600477104721,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "PSAzLVX5CsmT",
    "outputId": "e42dc9d4-2a46-4ea7-da8a-41422eee9c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNewsNet(\n",
      "  (embedding): Embedding(39578, 256)\n",
      "  (LSTM): LSTM(256, 300, batch_first=True, bidirectional=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (out): Linear(in_features=600, out_features=1, bias=True)\n",
      ")\n",
      "training ...\n",
      "Epoch [1/7], Step [2/1218], Train Loss: 0.8550, Valid Loss: 0.7061\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [4/1218], Train Loss: 0.6939, Valid Loss: 0.6672\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [6/1218], Train Loss: 0.6564, Valid Loss: 0.6801\n",
      "Epoch [1/7], Step [8/1218], Train Loss: 0.4592, Valid Loss: 1.4873\n",
      "Epoch [1/7], Step [10/1218], Train Loss: 1.3791, Valid Loss: 0.6523\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [12/1218], Train Loss: 0.7037, Valid Loss: 0.8266\n",
      "Epoch [1/7], Step [14/1218], Train Loss: 0.9253, Valid Loss: 0.6928\n",
      "Epoch [1/7], Step [16/1218], Train Loss: 0.7024, Valid Loss: 0.6436\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [18/1218], Train Loss: 0.6871, Valid Loss: 0.6548\n",
      "Epoch [1/7], Step [20/1218], Train Loss: 0.6523, Valid Loss: 0.6326\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [22/1218], Train Loss: 0.7059, Valid Loss: 0.6142\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [24/1218], Train Loss: 0.5324, Valid Loss: 0.6013\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [26/1218], Train Loss: 0.5131, Valid Loss: 0.5649\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [28/1218], Train Loss: 0.6470, Valid Loss: 0.5825\n",
      "Epoch [1/7], Step [30/1218], Train Loss: 0.5123, Valid Loss: 0.5395\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [32/1218], Train Loss: 0.4995, Valid Loss: 0.5205\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [34/1218], Train Loss: 0.5503, Valid Loss: 0.4884\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [36/1218], Train Loss: 0.4345, Valid Loss: 0.4740\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [38/1218], Train Loss: 0.7076, Valid Loss: 0.4337\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [40/1218], Train Loss: 0.4051, Valid Loss: 0.4711\n",
      "Epoch [1/7], Step [42/1218], Train Loss: 0.5164, Valid Loss: 0.5102\n",
      "Epoch [1/7], Step [44/1218], Train Loss: 0.5963, Valid Loss: 0.8480\n",
      "Epoch [1/7], Step [46/1218], Train Loss: 0.7230, Valid Loss: 0.4750\n",
      "Epoch [1/7], Step [48/1218], Train Loss: 0.4851, Valid Loss: 0.4628\n",
      "Epoch [1/7], Step [50/1218], Train Loss: 0.4675, Valid Loss: 0.4546\n",
      "Epoch [1/7], Step [52/1218], Train Loss: 0.7040, Valid Loss: 0.4756\n",
      "Epoch [1/7], Step [54/1218], Train Loss: 0.4787, Valid Loss: 0.4785\n",
      "Epoch [1/7], Step [56/1218], Train Loss: 0.4722, Valid Loss: 0.4215\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [58/1218], Train Loss: 0.5247, Valid Loss: 0.4160\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [60/1218], Train Loss: 0.2220, Valid Loss: 1.2894\n",
      "Epoch [1/7], Step [62/1218], Train Loss: 0.9252, Valid Loss: 0.6702\n",
      "Epoch [1/7], Step [64/1218], Train Loss: 0.6081, Valid Loss: 0.5371\n",
      "Epoch [1/7], Step [66/1218], Train Loss: 0.5592, Valid Loss: 0.4785\n",
      "Epoch [1/7], Step [68/1218], Train Loss: 0.5320, Valid Loss: 0.5361\n",
      "Epoch [1/7], Step [70/1218], Train Loss: 0.4352, Valid Loss: 0.6548\n",
      "Epoch [1/7], Step [72/1218], Train Loss: 0.6906, Valid Loss: 0.3678\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [74/1218], Train Loss: 0.4306, Valid Loss: 0.3772\n",
      "Epoch [1/7], Step [76/1218], Train Loss: 0.4819, Valid Loss: 0.3709\n",
      "Epoch [1/7], Step [78/1218], Train Loss: 0.2085, Valid Loss: 0.3749\n",
      "Epoch [1/7], Step [80/1218], Train Loss: 0.4637, Valid Loss: 0.3659\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [82/1218], Train Loss: 0.3876, Valid Loss: 0.3625\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [84/1218], Train Loss: 0.4076, Valid Loss: 0.3570\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [86/1218], Train Loss: 0.3082, Valid Loss: 0.3553\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [88/1218], Train Loss: 0.2443, Valid Loss: 0.3526\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [90/1218], Train Loss: 0.2525, Valid Loss: 0.3517\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [92/1218], Train Loss: 0.3266, Valid Loss: 0.3544\n",
      "Epoch [1/7], Step [94/1218], Train Loss: 0.2451, Valid Loss: 0.3712\n",
      "Epoch [1/7], Step [96/1218], Train Loss: 0.3441, Valid Loss: 0.3837\n",
      "Epoch [1/7], Step [98/1218], Train Loss: 0.5882, Valid Loss: 0.3519\n",
      "Epoch [1/7], Step [100/1218], Train Loss: 0.4056, Valid Loss: 0.3415\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [102/1218], Train Loss: 0.3288, Valid Loss: 0.3809\n",
      "Epoch [1/7], Step [104/1218], Train Loss: 0.4259, Valid Loss: 0.4084\n",
      "Epoch [1/7], Step [106/1218], Train Loss: 0.4643, Valid Loss: 0.3488\n",
      "Epoch [1/7], Step [108/1218], Train Loss: 0.3705, Valid Loss: 0.3495\n",
      "Epoch [1/7], Step [110/1218], Train Loss: 0.3836, Valid Loss: 0.3565\n",
      "Epoch [1/7], Step [112/1218], Train Loss: 0.3775, Valid Loss: 0.3555\n",
      "Epoch [1/7], Step [114/1218], Train Loss: 0.3389, Valid Loss: 0.3469\n",
      "Epoch [1/7], Step [116/1218], Train Loss: 0.2611, Valid Loss: 0.3445\n",
      "Epoch [1/7], Step [118/1218], Train Loss: 0.3920, Valid Loss: 0.3442\n",
      "Epoch [1/7], Step [120/1218], Train Loss: 0.2315, Valid Loss: 0.3577\n",
      "Epoch [1/7], Step [122/1218], Train Loss: 0.1720, Valid Loss: 0.3868\n",
      "Epoch [1/7], Step [124/1218], Train Loss: 0.7611, Valid Loss: 0.3745\n",
      "Epoch [1/7], Step [126/1218], Train Loss: 0.3017, Valid Loss: 0.3567\n",
      "Epoch [1/7], Step [128/1218], Train Loss: 0.2932, Valid Loss: 0.3414\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [130/1218], Train Loss: 0.3244, Valid Loss: 0.3364\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [132/1218], Train Loss: 0.2993, Valid Loss: 0.3594\n",
      "Epoch [1/7], Step [134/1218], Train Loss: 0.3176, Valid Loss: 0.3901\n",
      "Epoch [1/7], Step [136/1218], Train Loss: 0.3517, Valid Loss: 0.3896\n",
      "Epoch [1/7], Step [138/1218], Train Loss: 0.3694, Valid Loss: 0.3753\n",
      "Epoch [1/7], Step [140/1218], Train Loss: 0.3559, Valid Loss: 0.3467\n",
      "Epoch [1/7], Step [142/1218], Train Loss: 0.4554, Valid Loss: 0.3274\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [144/1218], Train Loss: 0.3469, Valid Loss: 0.3218\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [146/1218], Train Loss: 0.2908, Valid Loss: 0.3330\n",
      "Epoch [1/7], Step [148/1218], Train Loss: 0.4650, Valid Loss: 0.3438\n",
      "Epoch [1/7], Step [150/1218], Train Loss: 0.4127, Valid Loss: 0.3503\n",
      "Epoch [1/7], Step [152/1218], Train Loss: 0.3253, Valid Loss: 0.3681\n",
      "Epoch [1/7], Step [154/1218], Train Loss: 0.3318, Valid Loss: 0.3509\n",
      "Epoch [1/7], Step [156/1218], Train Loss: 0.3268, Valid Loss: 0.3241\n",
      "Epoch [1/7], Step [158/1218], Train Loss: 0.2368, Valid Loss: 0.3110\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [1/7], Step [160/1218], Train Loss: 0.4690, Valid Loss: 0.3173\n",
      "Epoch [1/7], Step [162/1218], Train Loss: 0.1686, Valid Loss: 0.3357\n",
      "Epoch [1/7], Step [164/1218], Train Loss: 0.2988, Valid Loss: 0.3499\n",
      "Epoch [1/7], Step [166/1218], Train Loss: 0.6775, Valid Loss: 0.3532\n",
      "Epoch [1/7], Step [168/1218], Train Loss: 0.3188, Valid Loss: 0.3382\n",
      "Epoch [1/7], Step [170/1218], Train Loss: 0.4139, Valid Loss: 0.3196\n",
      "Epoch [1/7], Step [172/1218], Train Loss: 0.2831, Valid Loss: 0.3124\n",
      "Epoch [1/7], Step [174/1218], Train Loss: 0.3530, Valid Loss: 0.3139\n",
      "Epoch [2/7], Step [176/1218], Train Loss: 0.1106, Valid Loss: 0.3162\n",
      "Epoch [2/7], Step [178/1218], Train Loss: 0.2698, Valid Loss: 0.3151\n",
      "Epoch [2/7], Step [180/1218], Train Loss: 0.3382, Valid Loss: 0.3117\n",
      "Epoch [2/7], Step [182/1218], Train Loss: 0.2240, Valid Loss: 0.3178\n",
      "Epoch [2/7], Step [184/1218], Train Loss: 0.1832, Valid Loss: 0.3293\n",
      "Epoch [2/7], Step [186/1218], Train Loss: 0.2769, Valid Loss: 0.3433\n",
      "Epoch [2/7], Step [188/1218], Train Loss: 0.2027, Valid Loss: 0.3489\n",
      "Epoch [2/7], Step [190/1218], Train Loss: 0.3969, Valid Loss: 0.3489\n",
      "Epoch [2/7], Step [192/1218], Train Loss: 0.3775, Valid Loss: 0.3349\n",
      "Epoch [2/7], Step [194/1218], Train Loss: 0.3428, Valid Loss: 0.3211\n",
      "Epoch [2/7], Step [196/1218], Train Loss: 0.3553, Valid Loss: 0.3158\n",
      "Epoch [2/7], Step [198/1218], Train Loss: 0.1907, Valid Loss: 0.3161\n",
      "Epoch [2/7], Step [200/1218], Train Loss: 0.3029, Valid Loss: 0.3189\n",
      "Epoch [2/7], Step [202/1218], Train Loss: 0.1516, Valid Loss: 0.3247\n",
      "Epoch [2/7], Step [204/1218], Train Loss: 0.2661, Valid Loss: 0.3385\n",
      "Epoch [2/7], Step [206/1218], Train Loss: 0.1408, Valid Loss: 0.3452\n",
      "Epoch [2/7], Step [208/1218], Train Loss: 0.1704, Valid Loss: 0.3478\n",
      "Epoch [2/7], Step [210/1218], Train Loss: 0.1424, Valid Loss: 0.3487\n",
      "Epoch [2/7], Step [212/1218], Train Loss: 0.4067, Valid Loss: 0.3291\n",
      "Epoch [2/7], Step [214/1218], Train Loss: 0.1475, Valid Loss: 0.3332\n",
      "Epoch [2/7], Step [216/1218], Train Loss: 0.4377, Valid Loss: 0.3943\n",
      "Epoch [2/7], Step [218/1218], Train Loss: 0.4539, Valid Loss: 0.4344\n",
      "Epoch [2/7], Step [220/1218], Train Loss: 0.4054, Valid Loss: 0.4844\n",
      "Epoch [2/7], Step [222/1218], Train Loss: 0.4584, Valid Loss: 0.4864\n",
      "Epoch [2/7], Step [224/1218], Train Loss: 0.4854, Valid Loss: 0.5049\n",
      "Epoch [2/7], Step [226/1218], Train Loss: 0.5189, Valid Loss: 0.4830\n",
      "Epoch [2/7], Step [228/1218], Train Loss: 0.4324, Valid Loss: 0.4744\n",
      "Epoch [2/7], Step [230/1218], Train Loss: 0.3463, Valid Loss: 0.4879\n",
      "Epoch [2/7], Step [232/1218], Train Loss: 0.5653, Valid Loss: 0.4376\n",
      "Epoch [2/7], Step [234/1218], Train Loss: 0.4325, Valid Loss: 0.4296\n",
      "Epoch [2/7], Step [236/1218], Train Loss: 0.4152, Valid Loss: 0.4506\n",
      "Epoch [2/7], Step [238/1218], Train Loss: 0.4340, Valid Loss: 0.3737\n",
      "Epoch [2/7], Step [240/1218], Train Loss: 0.3263, Valid Loss: 0.3591\n",
      "Epoch [2/7], Step [242/1218], Train Loss: 0.3320, Valid Loss: 0.4153\n",
      "Epoch [2/7], Step [244/1218], Train Loss: 0.2555, Valid Loss: 0.3972\n",
      "Epoch [2/7], Step [246/1218], Train Loss: 0.4232, Valid Loss: 0.3570\n",
      "Epoch [2/7], Step [248/1218], Train Loss: 0.2262, Valid Loss: 0.3492\n",
      "Epoch [2/7], Step [250/1218], Train Loss: 0.2947, Valid Loss: 0.3600\n",
      "Epoch [2/7], Step [252/1218], Train Loss: 0.5170, Valid Loss: 0.3925\n",
      "Epoch [2/7], Step [254/1218], Train Loss: 0.4002, Valid Loss: 0.4109\n",
      "Epoch [2/7], Step [256/1218], Train Loss: 0.1789, Valid Loss: 0.3944\n",
      "Epoch [2/7], Step [258/1218], Train Loss: 0.1981, Valid Loss: 0.3853\n",
      "Epoch [2/7], Step [260/1218], Train Loss: 0.3389, Valid Loss: 0.3695\n",
      "Epoch [2/7], Step [262/1218], Train Loss: 0.3701, Valid Loss: 0.3574\n",
      "Epoch [2/7], Step [264/1218], Train Loss: 0.2070, Valid Loss: 0.3490\n",
      "Epoch [2/7], Step [266/1218], Train Loss: 0.3115, Valid Loss: 0.3417\n",
      "Epoch [2/7], Step [268/1218], Train Loss: 0.3005, Valid Loss: 0.3402\n",
      "Epoch [2/7], Step [270/1218], Train Loss: 0.2688, Valid Loss: 0.3382\n",
      "Epoch [2/7], Step [272/1218], Train Loss: 0.2053, Valid Loss: 0.3357\n",
      "Epoch [2/7], Step [274/1218], Train Loss: 0.3802, Valid Loss: 0.3338\n",
      "Epoch [2/7], Step [276/1218], Train Loss: 0.4270, Valid Loss: 0.3290\n",
      "Epoch [2/7], Step [278/1218], Train Loss: 0.1668, Valid Loss: 0.3365\n",
      "Epoch [2/7], Step [280/1218], Train Loss: 0.2843, Valid Loss: 0.3440\n",
      "Epoch [2/7], Step [282/1218], Train Loss: 0.1756, Valid Loss: 0.3344\n",
      "Epoch [2/7], Step [284/1218], Train Loss: 0.1901, Valid Loss: 0.3276\n",
      "Epoch [2/7], Step [286/1218], Train Loss: 0.3310, Valid Loss: 0.3244\n",
      "Epoch [2/7], Step [288/1218], Train Loss: 0.3375, Valid Loss: 0.3230\n",
      "Epoch [2/7], Step [290/1218], Train Loss: 0.3971, Valid Loss: 0.3155\n",
      "Epoch [2/7], Step [292/1218], Train Loss: 0.2729, Valid Loss: 0.3122\n",
      "Epoch [2/7], Step [294/1218], Train Loss: 0.2958, Valid Loss: 0.3140\n",
      "Epoch [2/7], Step [296/1218], Train Loss: 0.2905, Valid Loss: 0.3155\n",
      "Epoch [2/7], Step [298/1218], Train Loss: 0.2816, Valid Loss: 0.3144\n",
      "Epoch [2/7], Step [300/1218], Train Loss: 0.2772, Valid Loss: 0.3187\n",
      "Epoch [2/7], Step [302/1218], Train Loss: 0.4009, Valid Loss: 0.3202\n",
      "Epoch [2/7], Step [304/1218], Train Loss: 0.4298, Valid Loss: 0.3137\n",
      "Epoch [2/7], Step [306/1218], Train Loss: 0.1813, Valid Loss: 0.3146\n",
      "Epoch [2/7], Step [308/1218], Train Loss: 0.3167, Valid Loss: 0.3157\n",
      "Epoch [2/7], Step [310/1218], Train Loss: 0.2090, Valid Loss: 0.3093\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [2/7], Step [312/1218], Train Loss: 0.2752, Valid Loss: 0.3051\n",
      "Model saved to :./saved/model.pt\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Epoch [2/7], Step [314/1218], Train Loss: 0.2317, Valid Loss: 0.3063\n",
      "Epoch [2/7], Step [316/1218], Train Loss: 0.2195, Valid Loss: 0.3173\n",
      "Epoch [2/7], Step [318/1218], Train Loss: 0.2797, Valid Loss: 0.3146\n",
      "Epoch [2/7], Step [320/1218], Train Loss: 0.3990, Valid Loss: 0.3099\n",
      "Epoch [2/7], Step [322/1218], Train Loss: 0.2001, Valid Loss: 0.3134\n",
      "Epoch [2/7], Step [324/1218], Train Loss: 0.2128, Valid Loss: 0.3120\n",
      "Epoch [2/7], Step [326/1218], Train Loss: 0.1234, Valid Loss: 0.3102\n",
      "Epoch [2/7], Step [328/1218], Train Loss: 0.1925, Valid Loss: 0.3100\n",
      "Epoch [2/7], Step [330/1218], Train Loss: 0.3481, Valid Loss: 0.3126\n",
      "Epoch [2/7], Step [332/1218], Train Loss: 0.1972, Valid Loss: 0.3544\n",
      "Epoch [2/7], Step [334/1218], Train Loss: 0.3593, Valid Loss: 0.3074\n",
      "Epoch [2/7], Step [336/1218], Train Loss: 0.2324, Valid Loss: 0.3051\n",
      "Epoch [2/7], Step [338/1218], Train Loss: 0.2095, Valid Loss: 0.3135\n",
      "Epoch [2/7], Step [340/1218], Train Loss: 0.4997, Valid Loss: 0.3179\n",
      "Epoch [2/7], Step [342/1218], Train Loss: 0.1392, Valid Loss: 0.3191\n",
      "Epoch [2/7], Step [344/1218], Train Loss: 0.2608, Valid Loss: 0.3231\n",
      "Epoch [2/7], Step [346/1218], Train Loss: 0.4989, Valid Loss: 0.3243\n",
      "Epoch [2/7], Step [348/1218], Train Loss: 0.2852, Valid Loss: 0.3217\n",
      "Epoch [3/7], Step [350/1218], Train Loss: 0.1868, Valid Loss: 0.3238\n",
      "Epoch [3/7], Step [352/1218], Train Loss: 0.2626, Valid Loss: 0.3262\n",
      "Epoch [3/7], Step [354/1218], Train Loss: 0.2945, Valid Loss: 0.3255\n",
      "Epoch [3/7], Step [356/1218], Train Loss: 0.1823, Valid Loss: 0.3239\n",
      "Epoch [3/7], Step [358/1218], Train Loss: 0.1622, Valid Loss: 0.3207\n",
      "Epoch [3/7], Step [360/1218], Train Loss: 0.1410, Valid Loss: 0.3153\n",
      "Epoch [3/7], Step [362/1218], Train Loss: 0.1801, Valid Loss: 0.3102\n",
      "Epoch [3/7], Step [364/1218], Train Loss: 0.1371, Valid Loss: 0.3087\n",
      "Epoch [3/7], Step [366/1218], Train Loss: 0.2699, Valid Loss: 0.3100\n",
      "Epoch [3/7], Step [368/1218], Train Loss: 0.2048, Valid Loss: 0.3133\n",
      "Epoch [3/7], Step [370/1218], Train Loss: 0.2939, Valid Loss: 0.3161\n",
      "Epoch [3/7], Step [372/1218], Train Loss: 0.1883, Valid Loss: 0.3178\n",
      "Epoch [3/7], Step [374/1218], Train Loss: 0.2159, Valid Loss: 0.3187\n",
      "Epoch [3/7], Step [376/1218], Train Loss: 0.2714, Valid Loss: 0.3288\n",
      "Epoch [3/7], Step [378/1218], Train Loss: 0.1661, Valid Loss: 0.3360\n",
      "Epoch [3/7], Step [380/1218], Train Loss: 0.1411, Valid Loss: 0.3371\n",
      "Epoch [3/7], Step [382/1218], Train Loss: 0.0970, Valid Loss: 0.3317\n",
      "Epoch [3/7], Step [384/1218], Train Loss: 0.2599, Valid Loss: 0.3279\n",
      "Epoch [3/7], Step [386/1218], Train Loss: 0.2461, Valid Loss: 0.3263\n",
      "Epoch [3/7], Step [388/1218], Train Loss: 0.1865, Valid Loss: 0.3208\n",
      "Epoch [3/7], Step [390/1218], Train Loss: 0.2014, Valid Loss: 0.3175\n",
      "Epoch [3/7], Step [392/1218], Train Loss: 0.3206, Valid Loss: 0.3167\n",
      "Epoch [3/7], Step [394/1218], Train Loss: 0.1331, Valid Loss: 0.3193\n",
      "Epoch [3/7], Step [396/1218], Train Loss: 0.2083, Valid Loss: 0.3214\n",
      "Epoch [3/7], Step [398/1218], Train Loss: 0.1693, Valid Loss: 0.3341\n",
      "Epoch [3/7], Step [400/1218], Train Loss: 0.0869, Valid Loss: 0.3369\n",
      "Epoch [3/7], Step [402/1218], Train Loss: 0.2088, Valid Loss: 0.3302\n",
      "Epoch [3/7], Step [404/1218], Train Loss: 0.1279, Valid Loss: 0.3245\n",
      "Epoch [3/7], Step [406/1218], Train Loss: 0.1454, Valid Loss: 0.3213\n",
      "Epoch [3/7], Step [408/1218], Train Loss: 0.1738, Valid Loss: 0.3254\n",
      "Epoch [3/7], Step [410/1218], Train Loss: 0.1895, Valid Loss: 0.3376\n",
      "Epoch [3/7], Step [412/1218], Train Loss: 0.1931, Valid Loss: 0.3389\n",
      "Epoch [3/7], Step [414/1218], Train Loss: 0.1835, Valid Loss: 0.3321\n",
      "Epoch [3/7], Step [416/1218], Train Loss: 0.1600, Valid Loss: 0.3275\n",
      "Epoch [3/7], Step [418/1218], Train Loss: 0.1013, Valid Loss: 0.3295\n",
      "Epoch [3/7], Step [420/1218], Train Loss: 0.1497, Valid Loss: 0.3343\n",
      "Epoch [3/7], Step [422/1218], Train Loss: 0.1167, Valid Loss: 0.3368\n",
      "Epoch [3/7], Step [424/1218], Train Loss: 0.2105, Valid Loss: 0.3346\n",
      "Epoch [3/7], Step [426/1218], Train Loss: 0.2173, Valid Loss: 0.3347\n",
      "Epoch [3/7], Step [428/1218], Train Loss: 0.1705, Valid Loss: 0.3339\n",
      "Epoch [3/7], Step [430/1218], Train Loss: 0.2058, Valid Loss: 0.3336\n",
      "Epoch [3/7], Step [432/1218], Train Loss: 0.1373, Valid Loss: 0.3310\n",
      "Epoch [3/7], Step [434/1218], Train Loss: 0.1740, Valid Loss: 0.3217\n",
      "Epoch [3/7], Step [436/1218], Train Loss: 0.1972, Valid Loss: 0.3201\n",
      "Epoch [3/7], Step [438/1218], Train Loss: 0.1531, Valid Loss: 0.3204\n",
      "Epoch [3/7], Step [440/1218], Train Loss: 0.1429, Valid Loss: 0.3230\n",
      "Epoch [3/7], Step [442/1218], Train Loss: 0.1729, Valid Loss: 0.3240\n",
      "Epoch [3/7], Step [444/1218], Train Loss: 0.1854, Valid Loss: 0.3259\n",
      "Epoch [3/7], Step [446/1218], Train Loss: 0.0931, Valid Loss: 0.3284\n",
      "Epoch [3/7], Step [448/1218], Train Loss: 0.2254, Valid Loss: 0.3307\n",
      "Epoch [3/7], Step [450/1218], Train Loss: 0.2525, Valid Loss: 0.3296\n",
      "Epoch [3/7], Step [452/1218], Train Loss: 0.2326, Valid Loss: 0.3279\n",
      "Epoch [3/7], Step [454/1218], Train Loss: 0.1741, Valid Loss: 0.3331\n",
      "Epoch [3/7], Step [456/1218], Train Loss: 0.1766, Valid Loss: 0.3372\n",
      "Epoch [3/7], Step [458/1218], Train Loss: 0.1740, Valid Loss: 0.3320\n",
      "Epoch [3/7], Step [460/1218], Train Loss: 0.0886, Valid Loss: 0.3266\n",
      "Epoch [3/7], Step [462/1218], Train Loss: 0.1043, Valid Loss: 0.3313\n",
      "Epoch [3/7], Step [464/1218], Train Loss: 0.0774, Valid Loss: 0.3421\n",
      "Epoch [3/7], Step [466/1218], Train Loss: 0.0903, Valid Loss: 0.3476\n",
      "Epoch [3/7], Step [468/1218], Train Loss: 0.1944, Valid Loss: 0.3482\n",
      "Epoch [3/7], Step [470/1218], Train Loss: 0.0698, Valid Loss: 0.4000\n",
      "Epoch [3/7], Step [472/1218], Train Loss: 0.2801, Valid Loss: 0.4119\n",
      "Epoch [3/7], Step [474/1218], Train Loss: 0.3206, Valid Loss: 0.3933\n",
      "Epoch [3/7], Step [476/1218], Train Loss: 0.2820, Valid Loss: 0.3815\n",
      "Epoch [3/7], Step [478/1218], Train Loss: 0.1448, Valid Loss: 0.3791\n",
      "Epoch [3/7], Step [480/1218], Train Loss: 0.1075, Valid Loss: 0.3796\n",
      "Epoch [3/7], Step [482/1218], Train Loss: 0.2604, Valid Loss: 0.3793\n",
      "Epoch [3/7], Step [484/1218], Train Loss: 0.2151, Valid Loss: 0.3775\n",
      "Epoch [3/7], Step [486/1218], Train Loss: 0.3375, Valid Loss: 0.3656\n",
      "Epoch [3/7], Step [488/1218], Train Loss: 0.2232, Valid Loss: 0.3635\n",
      "Epoch [3/7], Step [490/1218], Train Loss: 0.3073, Valid Loss: 0.3648\n",
      "Epoch [3/7], Step [492/1218], Train Loss: 0.1990, Valid Loss: 0.3578\n",
      "Epoch [3/7], Step [494/1218], Train Loss: 0.1442, Valid Loss: 0.3450\n",
      "Epoch [3/7], Step [496/1218], Train Loss: 0.2346, Valid Loss: 0.3372\n",
      "Epoch [3/7], Step [498/1218], Train Loss: 0.1280, Valid Loss: 0.3339\n",
      "Epoch [3/7], Step [500/1218], Train Loss: 0.2053, Valid Loss: 0.3333\n",
      "Epoch [3/7], Step [502/1218], Train Loss: 0.1566, Valid Loss: 0.3371\n",
      "Epoch [3/7], Step [504/1218], Train Loss: 0.1494, Valid Loss: 0.3514\n",
      "Epoch [3/7], Step [506/1218], Train Loss: 0.1958, Valid Loss: 0.3558\n",
      "Epoch [3/7], Step [508/1218], Train Loss: 0.1152, Valid Loss: 0.3535\n",
      "Epoch [3/7], Step [510/1218], Train Loss: 0.1834, Valid Loss: 0.3496\n",
      "Epoch [3/7], Step [512/1218], Train Loss: 0.2381, Valid Loss: 0.3410\n",
      "Epoch [3/7], Step [514/1218], Train Loss: 0.2608, Valid Loss: 0.3358\n",
      "Epoch [3/7], Step [516/1218], Train Loss: 0.1879, Valid Loss: 0.3414\n",
      "Epoch [3/7], Step [518/1218], Train Loss: 0.1479, Valid Loss: 0.3555\n",
      "Epoch [3/7], Step [520/1218], Train Loss: 0.2020, Valid Loss: 0.3717\n",
      "Epoch [3/7], Step [522/1218], Train Loss: 0.2262, Valid Loss: 0.3968\n",
      "Epoch [4/7], Step [524/1218], Train Loss: 0.1374, Valid Loss: 0.4092\n",
      "Epoch [4/7], Step [526/1218], Train Loss: 0.1396, Valid Loss: 0.3889\n",
      "Epoch [4/7], Step [528/1218], Train Loss: 0.0342, Valid Loss: 0.3682\n",
      "Epoch [4/7], Step [530/1218], Train Loss: 0.0593, Valid Loss: 0.3527\n",
      "Epoch [4/7], Step [532/1218], Train Loss: 0.1167, Valid Loss: 0.3433\n",
      "Epoch [4/7], Step [534/1218], Train Loss: 0.0919, Valid Loss: 0.3433\n",
      "Epoch [4/7], Step [536/1218], Train Loss: 0.0971, Valid Loss: 0.3507\n",
      "Epoch [4/7], Step [538/1218], Train Loss: 0.1573, Valid Loss: 0.3520\n",
      "Epoch [4/7], Step [540/1218], Train Loss: 0.1153, Valid Loss: 0.3517\n",
      "Epoch [4/7], Step [542/1218], Train Loss: 0.0272, Valid Loss: 0.3548\n",
      "Epoch [4/7], Step [544/1218], Train Loss: 0.1360, Valid Loss: 0.3605\n",
      "Epoch [4/7], Step [546/1218], Train Loss: 0.1285, Valid Loss: 0.3662\n",
      "Epoch [4/7], Step [548/1218], Train Loss: 0.1275, Valid Loss: 0.3671\n",
      "Epoch [4/7], Step [550/1218], Train Loss: 0.1594, Valid Loss: 0.3657\n",
      "Epoch [4/7], Step [552/1218], Train Loss: 0.0610, Valid Loss: 0.3636\n",
      "Epoch [4/7], Step [554/1218], Train Loss: 0.0781, Valid Loss: 0.3638\n",
      "Epoch [4/7], Step [556/1218], Train Loss: 0.0990, Valid Loss: 0.3658\n",
      "Epoch [4/7], Step [558/1218], Train Loss: 0.0493, Valid Loss: 0.3714\n",
      "Epoch [4/7], Step [560/1218], Train Loss: 0.1593, Valid Loss: 0.3738\n",
      "Epoch [4/7], Step [562/1218], Train Loss: 0.1735, Valid Loss: 0.3718\n",
      "Epoch [4/7], Step [564/1218], Train Loss: 0.0514, Valid Loss: 0.3699\n",
      "Epoch [4/7], Step [566/1218], Train Loss: 0.0813, Valid Loss: 0.3716\n",
      "Epoch [4/7], Step [568/1218], Train Loss: 0.0606, Valid Loss: 0.3703\n",
      "Epoch [4/7], Step [570/1218], Train Loss: 0.1698, Valid Loss: 0.3676\n",
      "Epoch [4/7], Step [572/1218], Train Loss: 0.0942, Valid Loss: 0.3671\n",
      "Epoch [4/7], Step [574/1218], Train Loss: 0.0241, Valid Loss: 0.3699\n",
      "Epoch [4/7], Step [576/1218], Train Loss: 0.0847, Valid Loss: 0.3728\n",
      "Epoch [4/7], Step [578/1218], Train Loss: 0.0815, Valid Loss: 0.3760\n",
      "Epoch [4/7], Step [580/1218], Train Loss: 0.0648, Valid Loss: 0.3795\n",
      "Epoch [4/7], Step [582/1218], Train Loss: 0.0613, Valid Loss: 0.3817\n",
      "Epoch [4/7], Step [584/1218], Train Loss: 0.1161, Valid Loss: 0.3835\n",
      "Epoch [4/7], Step [586/1218], Train Loss: 0.1267, Valid Loss: 0.3858\n",
      "Epoch [4/7], Step [588/1218], Train Loss: 0.1478, Valid Loss: 0.3879\n",
      "Epoch [4/7], Step [590/1218], Train Loss: 0.0874, Valid Loss: 0.3899\n",
      "Epoch [4/7], Step [592/1218], Train Loss: 0.1116, Valid Loss: 0.3905\n",
      "Epoch [4/7], Step [594/1218], Train Loss: 0.1411, Valid Loss: 0.3915\n",
      "Epoch [4/7], Step [596/1218], Train Loss: 0.1684, Valid Loss: 0.3920\n",
      "Epoch [4/7], Step [598/1218], Train Loss: 0.0541, Valid Loss: 0.3929\n",
      "Epoch [4/7], Step [600/1218], Train Loss: 0.1172, Valid Loss: 0.3978\n",
      "Epoch [4/7], Step [602/1218], Train Loss: 0.2472, Valid Loss: 0.4071\n",
      "Epoch [4/7], Step [604/1218], Train Loss: 0.1567, Valid Loss: 0.4048\n",
      "Epoch [4/7], Step [606/1218], Train Loss: 0.1181, Valid Loss: 0.3914\n",
      "Epoch [4/7], Step [608/1218], Train Loss: 0.1444, Valid Loss: 0.3902\n",
      "Epoch [4/7], Step [610/1218], Train Loss: 0.1348, Valid Loss: 0.4002\n",
      "Epoch [4/7], Step [612/1218], Train Loss: 0.0855, Valid Loss: 0.4138\n",
      "Epoch [4/7], Step [614/1218], Train Loss: 0.2024, Valid Loss: 0.4184\n",
      "Epoch [4/7], Step [616/1218], Train Loss: 0.0781, Valid Loss: 0.4118\n",
      "Epoch [4/7], Step [618/1218], Train Loss: 0.0518, Valid Loss: 0.4056\n",
      "Epoch [4/7], Step [620/1218], Train Loss: 0.2194, Valid Loss: 0.4039\n",
      "Epoch [4/7], Step [622/1218], Train Loss: 0.2027, Valid Loss: 0.4060\n",
      "Epoch [4/7], Step [624/1218], Train Loss: 0.0231, Valid Loss: 0.4079\n",
      "Epoch [4/7], Step [626/1218], Train Loss: 0.1095, Valid Loss: 0.4068\n",
      "Epoch [4/7], Step [628/1218], Train Loss: 0.0622, Valid Loss: 0.4069\n",
      "Epoch [4/7], Step [630/1218], Train Loss: 0.0499, Valid Loss: 0.4108\n",
      "Epoch [4/7], Step [632/1218], Train Loss: 0.0638, Valid Loss: 0.4182\n",
      "Epoch [4/7], Step [634/1218], Train Loss: 0.1614, Valid Loss: 0.4163\n",
      "Epoch [4/7], Step [636/1218], Train Loss: 0.1011, Valid Loss: 0.4197\n",
      "Epoch [4/7], Step [638/1218], Train Loss: 0.1592, Valid Loss: 0.4201\n",
      "Epoch [4/7], Step [640/1218], Train Loss: 0.0845, Valid Loss: 0.4186\n",
      "Epoch [4/7], Step [642/1218], Train Loss: 0.0805, Valid Loss: 0.4138\n",
      "Epoch [4/7], Step [644/1218], Train Loss: 0.0124, Valid Loss: 0.4131\n",
      "Epoch [4/7], Step [646/1218], Train Loss: 0.0462, Valid Loss: 0.4151\n",
      "Epoch [4/7], Step [648/1218], Train Loss: 0.1725, Valid Loss: 0.4123\n",
      "Epoch [4/7], Step [650/1218], Train Loss: 0.1473, Valid Loss: 0.4110\n",
      "Epoch [4/7], Step [652/1218], Train Loss: 0.1547, Valid Loss: 0.4142\n",
      "Epoch [4/7], Step [654/1218], Train Loss: 0.1013, Valid Loss: 0.4148\n",
      "Epoch [4/7], Step [656/1218], Train Loss: 0.0700, Valid Loss: 0.4172\n",
      "Epoch [4/7], Step [658/1218], Train Loss: 0.0761, Valid Loss: 0.4209\n",
      "Epoch [4/7], Step [660/1218], Train Loss: 0.0178, Valid Loss: 0.4264\n",
      "Epoch [4/7], Step [662/1218], Train Loss: 0.2152, Valid Loss: 0.4204\n",
      "Epoch [4/7], Step [664/1218], Train Loss: 0.1996, Valid Loss: 0.4146\n",
      "Epoch [4/7], Step [666/1218], Train Loss: 0.2235, Valid Loss: 0.4218\n",
      "Epoch [4/7], Step [668/1218], Train Loss: 0.0577, Valid Loss: 0.4422\n",
      "Epoch [4/7], Step [670/1218], Train Loss: 0.0504, Valid Loss: 0.4550\n",
      "Epoch [4/7], Step [672/1218], Train Loss: 0.1416, Valid Loss: 0.4424\n",
      "Epoch [4/7], Step [674/1218], Train Loss: 0.1791, Valid Loss: 0.4202\n",
      "Epoch [4/7], Step [676/1218], Train Loss: 0.0867, Valid Loss: 0.4193\n",
      "Epoch [4/7], Step [678/1218], Train Loss: 0.2005, Valid Loss: 0.4238\n",
      "Epoch [4/7], Step [680/1218], Train Loss: 0.1339, Valid Loss: 0.4257\n",
      "Epoch [4/7], Step [682/1218], Train Loss: 0.0561, Valid Loss: 0.4312\n",
      "Epoch [4/7], Step [684/1218], Train Loss: 0.1045, Valid Loss: 0.4327\n",
      "Epoch [4/7], Step [686/1218], Train Loss: 0.1955, Valid Loss: 0.4230\n",
      "Epoch [4/7], Step [688/1218], Train Loss: 0.0645, Valid Loss: 0.4207\n",
      "Epoch [4/7], Step [690/1218], Train Loss: 0.1611, Valid Loss: 0.4249\n",
      "Epoch [4/7], Step [692/1218], Train Loss: 0.1335, Valid Loss: 0.4314\n",
      "Epoch [4/7], Step [694/1218], Train Loss: 0.1200, Valid Loss: 0.4324\n",
      "Epoch [4/7], Step [696/1218], Train Loss: 0.1126, Valid Loss: 0.4291\n",
      "Epoch [5/7], Step [698/1218], Train Loss: 0.0353, Valid Loss: 0.4236\n",
      "Epoch [5/7], Step [700/1218], Train Loss: 0.0642, Valid Loss: 0.4156\n",
      "Epoch [5/7], Step [702/1218], Train Loss: 0.0420, Valid Loss: 0.4132\n",
      "Epoch [5/7], Step [704/1218], Train Loss: 0.0838, Valid Loss: 0.4133\n",
      "Epoch [5/7], Step [706/1218], Train Loss: 0.0725, Valid Loss: 0.4163\n",
      "Epoch [5/7], Step [708/1218], Train Loss: 0.0841, Valid Loss: 0.4200\n",
      "Epoch [5/7], Step [710/1218], Train Loss: 0.0243, Valid Loss: 0.4232\n",
      "Epoch [5/7], Step [712/1218], Train Loss: 0.0836, Valid Loss: 0.4271\n",
      "Epoch [5/7], Step [714/1218], Train Loss: 0.0547, Valid Loss: 0.4289\n",
      "Epoch [5/7], Step [716/1218], Train Loss: 0.0571, Valid Loss: 0.4286\n",
      "Epoch [5/7], Step [718/1218], Train Loss: 0.0370, Valid Loss: 0.4308\n",
      "Epoch [5/7], Step [720/1218], Train Loss: 0.0301, Valid Loss: 0.4343\n",
      "Epoch [5/7], Step [722/1218], Train Loss: 0.0374, Valid Loss: 0.4369\n",
      "Epoch [5/7], Step [724/1218], Train Loss: 0.0354, Valid Loss: 0.4398\n",
      "Epoch [5/7], Step [726/1218], Train Loss: 0.0570, Valid Loss: 0.4395\n",
      "Epoch [5/7], Step [728/1218], Train Loss: 0.0458, Valid Loss: 0.4398\n",
      "Epoch [5/7], Step [730/1218], Train Loss: 0.0199, Valid Loss: 0.4413\n",
      "Epoch [5/7], Step [732/1218], Train Loss: 0.0182, Valid Loss: 0.4424\n",
      "Epoch [5/7], Step [734/1218], Train Loss: 0.0407, Valid Loss: 0.4427\n",
      "Epoch [5/7], Step [736/1218], Train Loss: 0.0279, Valid Loss: 0.4423\n",
      "Epoch [5/7], Step [738/1218], Train Loss: 0.0220, Valid Loss: 0.4420\n",
      "Epoch [5/7], Step [740/1218], Train Loss: 0.0348, Valid Loss: 0.4422\n",
      "Epoch [5/7], Step [742/1218], Train Loss: 0.0460, Valid Loss: 0.4430\n",
      "Epoch [5/7], Step [744/1218], Train Loss: 0.0150, Valid Loss: 0.4448\n",
      "Epoch [5/7], Step [746/1218], Train Loss: 0.0528, Valid Loss: 0.4461\n",
      "Epoch [5/7], Step [748/1218], Train Loss: 0.0895, Valid Loss: 0.4459\n",
      "Epoch [5/7], Step [750/1218], Train Loss: 0.1253, Valid Loss: 0.4440\n",
      "Epoch [5/7], Step [752/1218], Train Loss: 0.0370, Valid Loss: 0.4422\n",
      "Epoch [5/7], Step [754/1218], Train Loss: 0.0378, Valid Loss: 0.4404\n",
      "Epoch [5/7], Step [756/1218], Train Loss: 0.0717, Valid Loss: 0.4384\n",
      "Epoch [5/7], Step [758/1218], Train Loss: 0.1029, Valid Loss: 0.4368\n",
      "Epoch [5/7], Step [760/1218], Train Loss: 0.0484, Valid Loss: 0.4368\n",
      "Epoch [5/7], Step [762/1218], Train Loss: 0.0205, Valid Loss: 0.4387\n",
      "Epoch [5/7], Step [764/1218], Train Loss: 0.0199, Valid Loss: 0.4401\n",
      "Epoch [5/7], Step [766/1218], Train Loss: 0.0586, Valid Loss: 0.4410\n",
      "Epoch [5/7], Step [768/1218], Train Loss: 0.0524, Valid Loss: 0.4437\n",
      "Epoch [5/7], Step [770/1218], Train Loss: 0.0827, Valid Loss: 0.4482\n",
      "Epoch [5/7], Step [772/1218], Train Loss: 0.0604, Valid Loss: 0.4513\n",
      "Epoch [5/7], Step [774/1218], Train Loss: 0.0375, Valid Loss: 0.4517\n",
      "Epoch [5/7], Step [776/1218], Train Loss: 0.0471, Valid Loss: 0.4520\n",
      "Epoch [5/7], Step [778/1218], Train Loss: 0.1298, Valid Loss: 0.4529\n",
      "Epoch [5/7], Step [780/1218], Train Loss: 0.0661, Valid Loss: 0.4555\n",
      "Epoch [5/7], Step [782/1218], Train Loss: 0.0832, Valid Loss: 0.4538\n",
      "Epoch [5/7], Step [784/1218], Train Loss: 0.0087, Valid Loss: 0.4541\n",
      "Epoch [5/7], Step [786/1218], Train Loss: 0.0607, Valid Loss: 0.4531\n",
      "Epoch [5/7], Step [788/1218], Train Loss: 0.0335, Valid Loss: 0.4534\n",
      "Epoch [5/7], Step [790/1218], Train Loss: 0.0480, Valid Loss: 0.4549\n",
      "Epoch [5/7], Step [792/1218], Train Loss: 0.0735, Valid Loss: 0.4557\n",
      "Epoch [5/7], Step [794/1218], Train Loss: 0.1511, Valid Loss: 0.4545\n",
      "Epoch [5/7], Step [796/1218], Train Loss: 0.0692, Valid Loss: 0.4604\n",
      "Epoch [5/7], Step [798/1218], Train Loss: 0.0526, Valid Loss: 0.4679\n",
      "Epoch [5/7], Step [800/1218], Train Loss: 0.1027, Valid Loss: 0.4763\n",
      "Epoch [5/7], Step [802/1218], Train Loss: 0.0738, Valid Loss: 0.4837\n",
      "Epoch [5/7], Step [804/1218], Train Loss: 0.0241, Valid Loss: 0.4853\n",
      "Epoch [5/7], Step [806/1218], Train Loss: 0.0673, Valid Loss: 0.4841\n",
      "Epoch [5/7], Step [808/1218], Train Loss: 0.0662, Valid Loss: 0.4729\n",
      "Epoch [5/7], Step [810/1218], Train Loss: 0.0770, Valid Loss: 0.4647\n",
      "Epoch [5/7], Step [812/1218], Train Loss: 0.0355, Valid Loss: 0.4626\n",
      "Epoch [5/7], Step [814/1218], Train Loss: 0.1215, Valid Loss: 0.4572\n",
      "Epoch [5/7], Step [816/1218], Train Loss: 0.0479, Valid Loss: 0.4556\n",
      "Epoch [5/7], Step [818/1218], Train Loss: 0.0582, Valid Loss: 0.4931\n",
      "Epoch [5/7], Step [820/1218], Train Loss: 0.0715, Valid Loss: 0.5742\n",
      "Epoch [5/7], Step [822/1218], Train Loss: 0.2267, Valid Loss: 0.5444\n",
      "Epoch [5/7], Step [824/1218], Train Loss: 0.0540, Valid Loss: 0.4810\n",
      "Epoch [5/7], Step [826/1218], Train Loss: 0.0246, Valid Loss: 0.4793\n",
      "Epoch [5/7], Step [828/1218], Train Loss: 0.1061, Valid Loss: 0.5101\n",
      "Epoch [5/7], Step [830/1218], Train Loss: 0.0695, Valid Loss: 0.5419\n",
      "Epoch [5/7], Step [832/1218], Train Loss: 0.0544, Valid Loss: 0.5545\n",
      "Epoch [5/7], Step [834/1218], Train Loss: 0.1372, Valid Loss: 0.5285\n",
      "Epoch [5/7], Step [836/1218], Train Loss: 0.1221, Valid Loss: 0.4982\n",
      "Epoch [5/7], Step [838/1218], Train Loss: 0.0396, Valid Loss: 0.4791\n",
      "Epoch [5/7], Step [840/1218], Train Loss: 0.0232, Valid Loss: 0.4763\n",
      "Epoch [5/7], Step [842/1218], Train Loss: 0.0934, Valid Loss: 0.4763\n",
      "Epoch [5/7], Step [844/1218], Train Loss: 0.1441, Valid Loss: 0.4769\n",
      "Epoch [5/7], Step [846/1218], Train Loss: 0.0461, Valid Loss: 0.4784\n",
      "Epoch [5/7], Step [848/1218], Train Loss: 0.0971, Valid Loss: 0.4780\n",
      "Epoch [5/7], Step [850/1218], Train Loss: 0.0556, Valid Loss: 0.4758\n",
      "Epoch [5/7], Step [852/1218], Train Loss: 0.1517, Valid Loss: 0.4745\n",
      "Epoch [5/7], Step [854/1218], Train Loss: 0.0889, Valid Loss: 0.4746\n",
      "Epoch [5/7], Step [856/1218], Train Loss: 0.0454, Valid Loss: 0.4756\n",
      "Epoch [5/7], Step [858/1218], Train Loss: 0.0765, Valid Loss: 0.4747\n",
      "Epoch [5/7], Step [860/1218], Train Loss: 0.0962, Valid Loss: 0.4757\n",
      "Epoch [5/7], Step [862/1218], Train Loss: 0.0823, Valid Loss: 0.4729\n",
      "Epoch [5/7], Step [864/1218], Train Loss: 0.0161, Valid Loss: 0.4698\n",
      "Epoch [5/7], Step [866/1218], Train Loss: 0.0443, Valid Loss: 0.4697\n",
      "Epoch [5/7], Step [868/1218], Train Loss: 0.1006, Valid Loss: 0.4724\n",
      "Epoch [5/7], Step [870/1218], Train Loss: 0.1069, Valid Loss: 0.4776\n",
      "Epoch [6/7], Step [872/1218], Train Loss: 0.0256, Valid Loss: 0.4819\n",
      "Epoch [6/7], Step [874/1218], Train Loss: 0.0660, Valid Loss: 0.4872\n",
      "Epoch [6/7], Step [876/1218], Train Loss: 0.0402, Valid Loss: 0.4945\n",
      "Epoch [6/7], Step [878/1218], Train Loss: 0.0222, Valid Loss: 0.4989\n",
      "Epoch [6/7], Step [880/1218], Train Loss: 0.0429, Valid Loss: 0.4995\n",
      "Epoch [6/7], Step [882/1218], Train Loss: 0.0065, Valid Loss: 0.5031\n",
      "Epoch [6/7], Step [884/1218], Train Loss: 0.0472, Valid Loss: 0.5071\n",
      "Epoch [6/7], Step [886/1218], Train Loss: 0.0172, Valid Loss: 0.5106\n",
      "Epoch [6/7], Step [888/1218], Train Loss: 0.0463, Valid Loss: 0.5131\n",
      "Epoch [6/7], Step [890/1218], Train Loss: 0.0139, Valid Loss: 0.5180\n",
      "Epoch [6/7], Step [892/1218], Train Loss: 0.0284, Valid Loss: 0.5207\n",
      "Epoch [6/7], Step [894/1218], Train Loss: 0.0855, Valid Loss: 0.5216\n",
      "Epoch [6/7], Step [896/1218], Train Loss: 0.0249, Valid Loss: 0.5195\n",
      "Epoch [6/7], Step [898/1218], Train Loss: 0.0160, Valid Loss: 0.5175\n",
      "Epoch [6/7], Step [900/1218], Train Loss: 0.0260, Valid Loss: 0.5197\n",
      "Epoch [6/7], Step [902/1218], Train Loss: 0.0835, Valid Loss: 0.5247\n",
      "Epoch [6/7], Step [904/1218], Train Loss: 0.0150, Valid Loss: 0.5332\n",
      "Epoch [6/7], Step [906/1218], Train Loss: 0.0606, Valid Loss: 0.5439\n",
      "Epoch [6/7], Step [908/1218], Train Loss: 0.0188, Valid Loss: 0.5492\n",
      "Epoch [6/7], Step [910/1218], Train Loss: 0.0287, Valid Loss: 0.5530\n",
      "Epoch [6/7], Step [912/1218], Train Loss: 0.0651, Valid Loss: 0.5524\n",
      "Epoch [6/7], Step [914/1218], Train Loss: 0.0311, Valid Loss: 0.5466\n",
      "Epoch [6/7], Step [916/1218], Train Loss: 0.0582, Valid Loss: 0.5382\n",
      "Epoch [6/7], Step [918/1218], Train Loss: 0.0222, Valid Loss: 0.5376\n",
      "Epoch [6/7], Step [920/1218], Train Loss: 0.0136, Valid Loss: 0.5376\n",
      "Epoch [6/7], Step [922/1218], Train Loss: 0.0091, Valid Loss: 0.5380\n",
      "Epoch [6/7], Step [924/1218], Train Loss: 0.0105, Valid Loss: 0.5399\n",
      "Epoch [6/7], Step [926/1218], Train Loss: 0.0857, Valid Loss: 0.5398\n",
      "Epoch [6/7], Step [928/1218], Train Loss: 0.0436, Valid Loss: 0.5451\n",
      "Epoch [6/7], Step [930/1218], Train Loss: 0.0301, Valid Loss: 0.5524\n",
      "Epoch [6/7], Step [932/1218], Train Loss: 0.0089, Valid Loss: 0.5579\n",
      "Epoch [6/7], Step [934/1218], Train Loss: 0.0276, Valid Loss: 0.5585\n",
      "Epoch [6/7], Step [936/1218], Train Loss: 0.0506, Valid Loss: 0.5546\n",
      "Epoch [6/7], Step [938/1218], Train Loss: 0.0537, Valid Loss: 0.5450\n",
      "Epoch [6/7], Step [940/1218], Train Loss: 0.0435, Valid Loss: 0.5401\n",
      "Epoch [6/7], Step [942/1218], Train Loss: 0.0217, Valid Loss: 0.5424\n",
      "Epoch [6/7], Step [944/1218], Train Loss: 0.0306, Valid Loss: 0.5461\n",
      "Epoch [6/7], Step [946/1218], Train Loss: 0.0533, Valid Loss: 0.5500\n",
      "Epoch [6/7], Step [948/1218], Train Loss: 0.1149, Valid Loss: 0.5536\n",
      "Epoch [6/7], Step [950/1218], Train Loss: 0.0165, Valid Loss: 0.5606\n",
      "Epoch [6/7], Step [952/1218], Train Loss: 0.0088, Valid Loss: 0.5678\n",
      "Epoch [6/7], Step [954/1218], Train Loss: 0.0134, Valid Loss: 0.5717\n",
      "Epoch [6/7], Step [956/1218], Train Loss: 0.0525, Valid Loss: 0.5669\n",
      "Epoch [6/7], Step [958/1218], Train Loss: 0.0227, Valid Loss: 0.5633\n",
      "Epoch [6/7], Step [960/1218], Train Loss: 0.0534, Valid Loss: 0.5669\n",
      "Epoch [6/7], Step [962/1218], Train Loss: 0.0059, Valid Loss: 0.5752\n",
      "Epoch [6/7], Step [964/1218], Train Loss: 0.0438, Valid Loss: 0.5840\n",
      "Epoch [6/7], Step [966/1218], Train Loss: 0.0183, Valid Loss: 0.5938\n",
      "Epoch [6/7], Step [968/1218], Train Loss: 0.0467, Valid Loss: 0.6015\n",
      "Epoch [6/7], Step [970/1218], Train Loss: 0.0187, Valid Loss: 0.6064\n",
      "Epoch [6/7], Step [972/1218], Train Loss: 0.0450, Valid Loss: 0.6116\n",
      "Epoch [6/7], Step [974/1218], Train Loss: 0.0625, Valid Loss: 0.6109\n",
      "Epoch [6/7], Step [976/1218], Train Loss: 0.0167, Valid Loss: 0.6049\n",
      "Epoch [6/7], Step [978/1218], Train Loss: 0.0669, Valid Loss: 0.5965\n",
      "Epoch [6/7], Step [980/1218], Train Loss: 0.0616, Valid Loss: 0.5882\n",
      "Epoch [6/7], Step [982/1218], Train Loss: 0.0978, Valid Loss: 0.5745\n",
      "Epoch [6/7], Step [984/1218], Train Loss: 0.0701, Valid Loss: 0.5704\n",
      "Epoch [6/7], Step [986/1218], Train Loss: 0.0538, Valid Loss: 0.5728\n",
      "Epoch [6/7], Step [988/1218], Train Loss: 0.0129, Valid Loss: 0.5755\n",
      "Epoch [6/7], Step [990/1218], Train Loss: 0.0247, Valid Loss: 0.5732\n",
      "Epoch [6/7], Step [992/1218], Train Loss: 0.0276, Valid Loss: 0.5659\n",
      "Epoch [6/7], Step [994/1218], Train Loss: 0.0351, Valid Loss: 0.5581\n",
      "Epoch [6/7], Step [996/1218], Train Loss: 0.0792, Valid Loss: 0.5525\n",
      "Epoch [6/7], Step [998/1218], Train Loss: 0.0183, Valid Loss: 0.5500\n",
      "Epoch [6/7], Step [1000/1218], Train Loss: 0.0359, Valid Loss: 0.5488\n",
      "Epoch [6/7], Step [1002/1218], Train Loss: 0.0182, Valid Loss: 0.5488\n",
      "Epoch [6/7], Step [1004/1218], Train Loss: 0.0478, Valid Loss: 0.5481\n",
      "Epoch [6/7], Step [1006/1218], Train Loss: 0.0723, Valid Loss: 0.5432\n",
      "Epoch [6/7], Step [1008/1218], Train Loss: 0.0204, Valid Loss: 0.5421\n",
      "Epoch [6/7], Step [1010/1218], Train Loss: 0.0202, Valid Loss: 0.5428\n",
      "Epoch [6/7], Step [1012/1218], Train Loss: 0.0541, Valid Loss: 0.5431\n",
      "Epoch [6/7], Step [1014/1218], Train Loss: 0.1515, Valid Loss: 0.5420\n",
      "Epoch [6/7], Step [1016/1218], Train Loss: 0.0535, Valid Loss: 0.5426\n",
      "Epoch [6/7], Step [1018/1218], Train Loss: 0.0666, Valid Loss: 0.5452\n",
      "Epoch [6/7], Step [1020/1218], Train Loss: 0.0493, Valid Loss: 0.5536\n",
      "Epoch [6/7], Step [1022/1218], Train Loss: 0.0348, Valid Loss: 0.5678\n",
      "Epoch [6/7], Step [1024/1218], Train Loss: 0.0492, Valid Loss: 0.5857\n",
      "Epoch [6/7], Step [1026/1218], Train Loss: 0.0294, Valid Loss: 0.6103\n",
      "Epoch [6/7], Step [1028/1218], Train Loss: 0.0477, Valid Loss: 0.6223\n",
      "Epoch [6/7], Step [1030/1218], Train Loss: 0.0469, Valid Loss: 0.6232\n",
      "Epoch [6/7], Step [1032/1218], Train Loss: 0.0977, Valid Loss: 0.5939\n",
      "Epoch [6/7], Step [1034/1218], Train Loss: 0.0865, Valid Loss: 0.5623\n",
      "Epoch [6/7], Step [1036/1218], Train Loss: 0.0780, Valid Loss: 0.5447\n",
      "Epoch [6/7], Step [1038/1218], Train Loss: 0.1767, Valid Loss: 0.5461\n",
      "Epoch [6/7], Step [1040/1218], Train Loss: 0.0512, Valid Loss: 0.5576\n",
      "Epoch [6/7], Step [1042/1218], Train Loss: 0.0943, Valid Loss: 0.5630\n",
      "Epoch [6/7], Step [1044/1218], Train Loss: 0.0273, Valid Loss: 0.5685\n",
      "Epoch [7/7], Step [1046/1218], Train Loss: 0.0362, Valid Loss: 0.5735\n",
      "Epoch [7/7], Step [1048/1218], Train Loss: 0.0457, Valid Loss: 0.5761\n",
      "Epoch [7/7], Step [1050/1218], Train Loss: 0.0256, Valid Loss: 0.5734\n",
      "Epoch [7/7], Step [1052/1218], Train Loss: 0.0460, Valid Loss: 0.5671\n",
      "Epoch [7/7], Step [1054/1218], Train Loss: 0.0223, Valid Loss: 0.5613\n",
      "Epoch [7/7], Step [1056/1218], Train Loss: 0.0430, Valid Loss: 0.5566\n",
      "Epoch [7/7], Step [1058/1218], Train Loss: 0.0230, Valid Loss: 0.5534\n",
      "Epoch [7/7], Step [1060/1218], Train Loss: 0.0293, Valid Loss: 0.5531\n",
      "Epoch [7/7], Step [1062/1218], Train Loss: 0.0074, Valid Loss: 0.5538\n",
      "Epoch [7/7], Step [1064/1218], Train Loss: 0.0196, Valid Loss: 0.5591\n",
      "Epoch [7/7], Step [1066/1218], Train Loss: 0.0251, Valid Loss: 0.5647\n",
      "Epoch [7/7], Step [1068/1218], Train Loss: 0.0172, Valid Loss: 0.5711\n",
      "Epoch [7/7], Step [1070/1218], Train Loss: 0.0253, Valid Loss: 0.5771\n",
      "Epoch [7/7], Step [1072/1218], Train Loss: 0.0066, Valid Loss: 0.5840\n",
      "Epoch [7/7], Step [1074/1218], Train Loss: 0.0072, Valid Loss: 0.5902\n",
      "Epoch [7/7], Step [1076/1218], Train Loss: 0.0202, Valid Loss: 0.5964\n",
      "Epoch [7/7], Step [1078/1218], Train Loss: 0.0068, Valid Loss: 0.6017\n",
      "Epoch [7/7], Step [1080/1218], Train Loss: 0.0167, Valid Loss: 0.6061\n",
      "Epoch [7/7], Step [1082/1218], Train Loss: 0.0198, Valid Loss: 0.6078\n",
      "Epoch [7/7], Step [1084/1218], Train Loss: 0.0087, Valid Loss: 0.6093\n",
      "Epoch [7/7], Step [1086/1218], Train Loss: 0.0163, Valid Loss: 0.6133\n",
      "Epoch [7/7], Step [1088/1218], Train Loss: 0.0556, Valid Loss: 0.6088\n",
      "Epoch [7/7], Step [1090/1218], Train Loss: 0.0121, Valid Loss: 0.6475\n",
      "Epoch [7/7], Step [1092/1218], Train Loss: 0.0572, Valid Loss: 0.7086\n",
      "Epoch [7/7], Step [1094/1218], Train Loss: 0.0490, Valid Loss: 0.6962\n",
      "Epoch [7/7], Step [1096/1218], Train Loss: 0.0262, Valid Loss: 0.6471\n",
      "Epoch [7/7], Step [1098/1218], Train Loss: 0.0220, Valid Loss: 0.6202\n",
      "Epoch [7/7], Step [1100/1218], Train Loss: 0.0266, Valid Loss: 0.6124\n",
      "Epoch [7/7], Step [1102/1218], Train Loss: 0.0461, Valid Loss: 0.6179\n",
      "Epoch [7/7], Step [1104/1218], Train Loss: 0.0883, Valid Loss: 0.6260\n",
      "Epoch [7/7], Step [1106/1218], Train Loss: 0.0876, Valid Loss: 0.6275\n",
      "Epoch [7/7], Step [1108/1218], Train Loss: 0.0135, Valid Loss: 0.6208\n",
      "Epoch [7/7], Step [1110/1218], Train Loss: 0.0661, Valid Loss: 0.6166\n",
      "Epoch [7/7], Step [1112/1218], Train Loss: 0.0430, Valid Loss: 0.6157\n",
      "Epoch [7/7], Step [1114/1218], Train Loss: 0.0328, Valid Loss: 0.6222\n",
      "Epoch [7/7], Step [1116/1218], Train Loss: 0.1019, Valid Loss: 0.6246\n",
      "Epoch [7/7], Step [1118/1218], Train Loss: 0.0467, Valid Loss: 0.6249\n",
      "Epoch [7/7], Step [1120/1218], Train Loss: 0.0265, Valid Loss: 0.6258\n",
      "Epoch [7/7], Step [1122/1218], Train Loss: 0.0128, Valid Loss: 0.6261\n",
      "Epoch [7/7], Step [1124/1218], Train Loss: 0.0338, Valid Loss: 0.6192\n",
      "Epoch [7/7], Step [1126/1218], Train Loss: 0.0304, Valid Loss: 0.6144\n",
      "Epoch [7/7], Step [1128/1218], Train Loss: 0.0528, Valid Loss: 0.6071\n",
      "Epoch [7/7], Step [1130/1218], Train Loss: 0.0158, Valid Loss: 0.6231\n",
      "Epoch [7/7], Step [1132/1218], Train Loss: 0.0203, Valid Loss: 0.6248\n",
      "Epoch [7/7], Step [1134/1218], Train Loss: 0.0356, Valid Loss: 0.6272\n",
      "Epoch [7/7], Step [1136/1218], Train Loss: 0.0520, Valid Loss: 0.6261\n",
      "Epoch [7/7], Step [1138/1218], Train Loss: 0.0386, Valid Loss: 0.6081\n",
      "Epoch [7/7], Step [1140/1218], Train Loss: 0.0247, Valid Loss: 0.6082\n",
      "Epoch [7/7], Step [1142/1218], Train Loss: 0.0204, Valid Loss: 0.6096\n",
      "Epoch [7/7], Step [1144/1218], Train Loss: 0.0221, Valid Loss: 0.6111\n",
      "Epoch [7/7], Step [1146/1218], Train Loss: 0.0167, Valid Loss: 0.6136\n",
      "Epoch [7/7], Step [1148/1218], Train Loss: 0.0290, Valid Loss: 0.6185\n",
      "Epoch [7/7], Step [1150/1218], Train Loss: 0.0270, Valid Loss: 0.6241\n",
      "Epoch [7/7], Step [1152/1218], Train Loss: 0.0192, Valid Loss: 0.6279\n",
      "Epoch [7/7], Step [1154/1218], Train Loss: 0.0133, Valid Loss: 0.6295\n",
      "Epoch [7/7], Step [1156/1218], Train Loss: 0.0281, Valid Loss: 0.6326\n",
      "Epoch [7/7], Step [1158/1218], Train Loss: 0.0186, Valid Loss: 0.6353\n",
      "Epoch [7/7], Step [1160/1218], Train Loss: 0.0216, Valid Loss: 0.6365\n",
      "Epoch [7/7], Step [1162/1218], Train Loss: 0.0535, Valid Loss: 0.6559\n",
      "Epoch [7/7], Step [1164/1218], Train Loss: 0.0481, Valid Loss: 0.6634\n",
      "Epoch [7/7], Step [1166/1218], Train Loss: 0.0169, Valid Loss: 0.6713\n",
      "Epoch [7/7], Step [1168/1218], Train Loss: 0.0530, Valid Loss: 0.6902\n",
      "Epoch [7/7], Step [1170/1218], Train Loss: 0.0509, Valid Loss: 0.6897\n",
      "Epoch [7/7], Step [1172/1218], Train Loss: 0.0922, Valid Loss: 0.6908\n",
      "Epoch [7/7], Step [1174/1218], Train Loss: 0.0151, Valid Loss: 0.6928\n",
      "Epoch [7/7], Step [1176/1218], Train Loss: 0.0267, Valid Loss: 0.6993\n",
      "Epoch [7/7], Step [1178/1218], Train Loss: 0.0721, Valid Loss: 0.7038\n",
      "Epoch [7/7], Step [1180/1218], Train Loss: 0.0581, Valid Loss: 0.6859\n",
      "Epoch [7/7], Step [1182/1218], Train Loss: 0.0003, Valid Loss: 0.6912\n",
      "Epoch [7/7], Step [1184/1218], Train Loss: 0.0336, Valid Loss: 0.6943\n",
      "Epoch [7/7], Step [1186/1218], Train Loss: 0.0239, Valid Loss: 0.6960\n",
      "Epoch [7/7], Step [1188/1218], Train Loss: 0.0451, Valid Loss: 0.6716\n",
      "Epoch [7/7], Step [1190/1218], Train Loss: 0.0107, Valid Loss: 0.6676\n",
      "Epoch [7/7], Step [1192/1218], Train Loss: 0.0193, Valid Loss: 0.6668\n",
      "Epoch [7/7], Step [1194/1218], Train Loss: 0.0407, Valid Loss: 0.6679\n",
      "Epoch [7/7], Step [1196/1218], Train Loss: 0.0474, Valid Loss: 0.6761\n",
      "Epoch [7/7], Step [1198/1218], Train Loss: 0.0987, Valid Loss: 0.6726\n",
      "Epoch [7/7], Step [1200/1218], Train Loss: 0.0668, Valid Loss: 0.6773\n",
      "Epoch [7/7], Step [1202/1218], Train Loss: 0.0368, Valid Loss: 0.6878\n",
      "Epoch [7/7], Step [1204/1218], Train Loss: 0.0303, Valid Loss: 0.6971\n",
      "Epoch [7/7], Step [1206/1218], Train Loss: 0.0786, Valid Loss: 0.7210\n",
      "Epoch [7/7], Step [1208/1218], Train Loss: 0.0218, Valid Loss: 0.7281\n",
      "Epoch [7/7], Step [1210/1218], Train Loss: 0.0655, Valid Loss: 0.7216\n",
      "Epoch [7/7], Step [1212/1218], Train Loss: 0.0881, Valid Loss: 0.7147\n",
      "Epoch [7/7], Step [1214/1218], Train Loss: 0.0215, Valid Loss: 0.7081\n",
      "Epoch [7/7], Step [1216/1218], Train Loss: 0.0063, Valid Loss: 0.7056\n",
      "Epoch [7/7], Step [1218/1218], Train Loss: 0.0611, Valid Loss: 0.7006\n",
      "Model saved to: ./saved/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 100,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = './saved',\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    print(\"training ...\")\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in train_loader:           \n",
    "            labels = labels.to(device)\n",
    "            titletext = titletext.to(device)\n",
    "            titletext_len = titletext_len.to(device)\n",
    "            output = model(titletext, titletext_len)\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                  # validation loop\n",
    "                  for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in valid_loader:\n",
    "                      labels = labels.to(device)\n",
    "                      titletext = titletext.to(device)\n",
    "                      titletext_len = titletext_len.to(device)\n",
    "                      output = model(titletext, titletext_len)\n",
    "\n",
    "                      loss = criterion(output, labels)\n",
    "                      valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')\n",
    "\n",
    "\n",
    "model = FakeNewsNet(hidden_size=300,num_layers=1,bi_lstm=True).to(device)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01,eps=1e-6,)\n",
    "\n",
    "train(model=model, optimizer=optimizer, num_epochs=7,eval_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2619747,
     "status": "ok",
     "timestamp": 1600477104729,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "hu-nVDaiCsmZ",
    "outputId": "220e18c6-1687-412d-aefc-6ff0daf42b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: ./saved/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics( './saved/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2625896,
     "status": "ok",
     "timestamp": 1600477110891,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "28-szrsaroUa",
    "outputId": "2908818e-76d3-44aa-fb58-506e3fd39108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNewsNet(\n",
      "  (embedding): Embedding(39578, 256)\n",
      "  (LSTM): LSTM(256, 300, batch_first=True, bidirectional=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (out): Linear(in_features=600, out_features=1, bias=True)\n",
      ")\n",
      "Model loaded from : ./saved/model.pt\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8640    0.8734    0.8687      3010\n",
      "           0     0.8981    0.8902    0.8942      3772\n",
      "\n",
      "    accuracy                         0.8828      6782\n",
      "   macro avg     0.8810    0.8818    0.8814      6782\n",
      "weighted avg     0.8829    0.8828    0.8828      6782\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate(model, test_loader, version='titletext', threshold=0.5):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in test_loader:           \n",
    "            labels = labels.to(device)\n",
    "            titletext = titletext.to(device)\n",
    "        \n",
    "            titletext_len = titletext_len.to(device)\n",
    "            output = model(titletext, titletext_len)\n",
    "\n",
    "            output = (output > threshold).int()\n",
    "            y_pred.extend(output.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "    \n",
    "    \n",
    "best_model = FakeNewsNet(hidden_size=300,num_layers=1,bi_lstm=True).to(device)\n",
    "print(best_model)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.01)\n",
    "\n",
    "load_checkpoint('./saved/model.pt', best_model, optimizer)\n",
    "evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2625887,
     "status": "ok",
     "timestamp": 1600477110894,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "PkMRsrI_r5Xj"
   },
   "outputs": [],
   "source": [
    "def predict(model, sentence):\n",
    "    from sacremoses import MosesTokenizer\n",
    "    mt = MosesTokenizer(lang='en')\n",
    "    tokenized = [tok for tok in mt.tokenize(sentence)]  #tokenize the sentence \n",
    "    indexed = [text_field.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
    "    length = [len(indexed)]                                    #compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
    "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
    "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
    "    prediction = model(tensor, length_tensor)                  #prediction \n",
    "    return prediction.item()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2625878,
     "status": "ok",
     "timestamp": 1600477110895,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "G1FbX0WmNVb1",
    "outputId": "8adc864d-811e-4c3e-cf7c-11a8b720f5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46486130356788635\n"
     ]
    }
   ],
   "source": [
    "best_model = FakeNewsNet().to(device)\n",
    "print(predict(best_model,\"Russian warships launched a massive missile attack on the terrorists near Aleppo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2625868,
     "status": "ok",
     "timestamp": 1600477110896,
     "user": {
      "displayName": "hakim",
      "photoUrl": "",
      "userId": "07988131915154974265"
     },
     "user_tz": -120
    },
    "id": "_xh_pVq4O0Rs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FakeNews_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit8c854b8a7a50472b80a9dffc58fcbb8a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
